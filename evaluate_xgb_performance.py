"""
XGBoost ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ëª¨ë“ˆ
"""

import pandas as pd
import numpy as np
import os
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib

plt.rcParams['font.family'] = ['Malgun Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class XGBoostEvaluator:
    def __init__(self, model_dir: str = "./xgb_results"):
        self.model_dir = model_dir
        self.models = {}
        self.load_models()
    
    def load_models(self):
        """ì €ì¥ëœ XGBoost ëª¨ë¸ë“¤ì„ ë¡œë“œ"""
        print(f"ğŸ“ ëª¨ë¸ ë¡œë”©: {self.model_dir}")
        
        if not os.path.exists(self.model_dir):
            print(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.model_dir}")
            return
        
        model_files = [f for f in os.listdir(self.model_dir) if f.startswith('xgb_') and f.endswith('.pkl')]
        
        for model_file in model_files:
            target_name = model_file.replace('xgb_', '').replace('.pkl', '')
            model_path = os.path.join(self.model_dir, model_file)
            
            try:
                self.models[target_name] = joblib.load(model_path)
                print(f"  âœ… {target_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"  âŒ {target_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    def evaluate_predictions(self, predictions_path: str = "./xgb_predictions.csv") -> Dict:
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê°€"""
        print("=== XGBoost ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ ===\n")
        
        if not os.path.exists(predictions_path):
            print(f"âŒ ì˜ˆì¸¡ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {predictions_path}")
            return {}
        
        df = pd.read_csv(predictions_path)
        print(f"ğŸ“Š ì˜ˆì¸¡ ë°ì´í„°: {len(df)}ê°œ ìƒ˜í”Œ")
        
        # íƒ€ê²Ÿë³„ ì„±ëŠ¥ ê³„ì‚°
        evaluation_results = {}
        
        for target in self.models.keys():
            true_col = f"true_{target}"
            pred_col = f"pred_{target}"
            
            if true_col in df.columns and pred_col in df.columns:
                y_true = df[true_col].values
                y_pred = df[pred_col].values
                
                # ê²°ì¸¡ì¹˜ ì œê±°
                mask = ~(np.isnan(y_true) | np.isnan(y_pred))
                y_true_clean = y_true[mask]
                y_pred_clean = y_pred[mask]
                
                if len(y_true_clean) > 0:
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
                    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))
                    mae = mean_absolute_error(y_true_clean, y_pred_clean)
                    r2 = r2_score(y_true_clean, y_pred_clean)
                    
                    # ì¶”ê°€ ë©”íŠ¸ë¦­
                    mape = np.mean(np.abs((y_true_clean - y_pred_clean) / y_true_clean)) * 100
                    residuals = y_true_clean - y_pred_clean
                    
                    evaluation_results[target] = {
                        'n_samples': len(y_true_clean),
                        'rmse': rmse,
                        'mae': mae,
                        'r2': r2,
                        'mape': mape,
                        'mean_true': np.mean(y_true_clean),
                        'std_true': np.std(y_true_clean),
                        'mean_pred': np.mean(y_pred_clean),
                        'std_pred': np.std(y_pred_clean),
                        'residuals_mean': np.mean(residuals),
                        'residuals_std': np.std(residuals)
                    }
                    
                    print(f"ğŸ¯ {target}:")
                    print(f"   RMSE: {rmse:.4f}")
                    print(f"   MAE:  {mae:.4f}")
                    print(f"   RÂ²:   {r2:.4f}")
                    print(f"   MAPE: {mape:.2f}%")
                    print(f"   ìƒ˜í”Œ: {len(y_true_clean)}ê°œ\n")
        
        return evaluation_results
    
    def create_performance_plots(self, predictions_path: str = "./xgb_predictions.csv", 
                               output_dir: str = "./xgb_evaluation"):
        """ì„±ëŠ¥ ì‹œê°í™” í”Œë¡¯ ìƒì„±"""
        print("=== ì„±ëŠ¥ ì‹œê°í™” ìƒì„± ===\n")
        
        os.makedirs(output_dir, exist_ok=True)
        
        if not os.path.exists(predictions_path):
            print(f"âŒ ì˜ˆì¸¡ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {predictions_path}")
            return
        
        df = pd.read_csv(predictions_path)
        
        for target in self.models.keys():
            true_col = f"true_{target}"
            pred_col = f"pred_{target}"
            
            if true_col in df.columns and pred_col in df.columns:
                y_true = df[true_col].values
                y_pred = df[pred_col].values
                
                # ê²°ì¸¡ì¹˜ ì œê±°
                mask = ~(np.isnan(y_true) | np.isnan(y_pred))
                y_true_clean = y_true[mask]
                y_pred_clean = y_pred[mask]
                
                if len(y_true_clean) > 0:
                    # í”Œë¡¯ ìƒì„±
                    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
                    fig.suptitle(f'{target} ì˜ˆì¸¡ ì„±ëŠ¥ ë¶„ì„', fontsize=16, fontweight='bold')
                    
                    # 1. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‚°ì ë„
                    axes[0, 0].scatter(y_true_clean, y_pred_clean, alpha=0.6, s=50)
                    axes[0, 0].plot([y_true_clean.min(), y_true_clean.max()], 
                                   [y_true_clean.min(), y_true_clean.max()], 'r--', lw=2)
                    axes[0, 0].set_xlabel('ì‹¤ì œê°’')
                    axes[0, 0].set_ylabel('ì˜ˆì¸¡ê°’')
                    axes[0, 0].set_title('ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’')
                    
                    # RÂ² í‘œì‹œ
                    r2 = r2_score(y_true_clean, y_pred_clean)
                    axes[0, 0].text(0.05, 0.95, f'RÂ² = {r2:.3f}', 
                                   transform=axes[0, 0].transAxes, 
                                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                    
                    # 2. ì”ì°¨ í”Œë¡¯
                    residuals = y_true_clean - y_pred_clean
                    axes[0, 1].scatter(y_pred_clean, residuals, alpha=0.6, s=50)
                    axes[0, 1].axhline(y=0, color='r', linestyle='--')
                    axes[0, 1].set_xlabel('ì˜ˆì¸¡ê°’')
                    axes[0, 1].set_ylabel('ì”ì°¨ (ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’)')
                    axes[0, 1].set_title('ì”ì°¨ í”Œë¡¯')
                    
                    # 3. ì”ì°¨ íˆìŠ¤í† ê·¸ë¨
                    axes[1, 0].hist(residuals, bins=20, alpha=0.7, edgecolor='black')
                    axes[1, 0].axvline(x=0, color='r', linestyle='--')
                    axes[1, 0].set_xlabel('ì”ì°¨')
                    axes[1, 0].set_ylabel('ë¹ˆë„')
                    axes[1, 0].set_title('ì”ì°¨ ë¶„í¬')
                    
                    # 4. ì˜ˆì¸¡ê°’ ë¶„í¬ ë¹„êµ
                    axes[1, 1].hist(y_true_clean, bins=15, alpha=0.7, label='ì‹¤ì œê°’', color='blue')
                    axes[1, 1].hist(y_pred_clean, bins=15, alpha=0.7, label='ì˜ˆì¸¡ê°’', color='red')
                    axes[1, 1].set_xlabel('ê°’')
                    axes[1, 1].set_ylabel('ë¹ˆë„')
                    axes[1, 1].set_title('ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ë¶„í¬')
                    axes[1, 1].legend()
                    
                    plt.tight_layout()
                    
                    # ì €ì¥
                    plot_path = os.path.join(output_dir, f"{target}_performance.png")
                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                    plt.close()
                    
                    print(f"ğŸ“ˆ {target} í”Œë¡¯ ì €ì¥: {plot_path}")
    
    def feature_importance_analysis(self, output_dir: str = "./xgb_evaluation"):
        """í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„"""
        print("\n=== í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ===\n")
        
        os.makedirs(output_dir, exist_ok=True)
        
        all_importance = {}
        
        for target, model in self.models.items():
            if hasattr(model, 'feature_importances_'):
                importance = model.feature_importances_
                all_importance[target] = importance
                
                # ìƒìœ„ 20ê°œ í”¼ì²˜ í”Œë¡¯
                top_indices = np.argsort(importance)[-20:]
                top_importance = importance[top_indices]
                top_features = [f'f{i}' for i in top_indices]
                
                plt.figure(figsize=(10, 8))
                plt.barh(range(len(top_features)), top_importance)
                plt.yticks(range(len(top_features)), top_features)
                plt.xlabel('Feature Importance')
                plt.title(f'{target} - Top 20 Feature Importance')
                plt.tight_layout()
                
                importance_path = os.path.join(output_dir, f"{target}_feature_importance.png")
                plt.savefig(importance_path, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"ğŸ“Š {target} í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥: {importance_path}")
        
        return all_importance
    
    def comprehensive_evaluation_report(self, embedding_path: str = "./runs_vit_light/test_embeddings.parquet",
                                      predictions_path: str = "./xgb_predictions.csv",
                                      output_dir: str = "./xgb_evaluation"):
        """ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("=== ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ===\n")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ì„±ëŠ¥ í‰ê°€
        evaluation_results = self.evaluate_predictions(predictions_path)
        
        # 2. ì‹œê°í™” ìƒì„±
        self.create_performance_plots(predictions_path, output_dir)
        
        # 3. í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
        feature_importance = self.feature_importance_analysis(output_dir)
        
        # 4. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'evaluation_results': evaluation_results,
            'summary': {
                'n_targets': len(evaluation_results),
                'targets': list(evaluation_results.keys()),
                'avg_r2': np.mean([r['r2'] for r in evaluation_results.values()]) if evaluation_results else 0,
                'avg_rmse': np.mean([r['rmse'] for r in evaluation_results.values()]) if evaluation_results else 0
            }
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = os.path.join(output_dir, "comprehensive_evaluation_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\n=== ì„±ëŠ¥ ìš”ì•½ ===")
        print(f"í‰ê°€ íƒ€ê²Ÿ ìˆ˜: {len(evaluation_results)}")
        if evaluation_results:
            print(f"í‰ê·  RÂ²: {report['summary']['avg_r2']:.4f}")
            print(f"í‰ê·  RMSE: {report['summary']['avg_rmse']:.4f}")
            
            print(f"\níƒ€ê²Ÿë³„ RÂ² ìˆœìœ„:")
            sorted_targets = sorted(evaluation_results.items(), key=lambda x: x[1]['r2'], reverse=True)
            for i, (target, metrics) in enumerate(sorted_targets, 1):
                print(f"  {i}. {target}: RÂ²={metrics['r2']:.4f}, RMSE={metrics['rmse']:.4f}")
        
        return report

def main():
    """ì‹¤í—˜ ì‹¤í–‰"""
    print("ğŸ§ª XGBoost ì„±ëŠ¥ í‰ê°€ ì‹¤í—˜ ì‹œì‘\n")
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = XGBoostEvaluator()
    
    if not evaluator.models:
        print("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € XGBoost í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # ì¢…í•© í‰ê°€ ì‹¤í–‰
    report = evaluator.comprehensive_evaluation_report()
    
    print(f"\nğŸ‰ í‰ê°€ ì™„ë£Œ! ê²°ê³¼ëŠ” './xgb_evaluation' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()